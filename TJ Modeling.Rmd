---
title: "TJ Modeling"
author: "Tim Morales"
date: "11/20/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}

df<-read.csv("/Users/timmorales/Desktop/STAT 6341/Tommy John Project/Data/ModelReadyData.csv")
df<- df[,-1]

#train test split
library(DMwR)
library(caret)
library(umap)
library(dplyr)
library(recipes)
set.seed(23)
train.indx <- createDataPartition(df$TJ_Yes, p = .7, list = F)

drop.vars<-c("Years.Between", "Repeat_Injury","TJ_No" )

train.set<-data.frame(df[train.indx, -which(names(df) %in% drop.vars)])

test.set<-data.frame(df[-train.indx,-which(names(df) %in% drop.vars)])

#center and scale numerics
train.set[,c(1:90)]<- lapply(train.set[,c(1:90)], scale)

#center and scale numerics
test.set[,c(1:90)]<- lapply(test.set[,c(1:90)], scale)

```



```{r}
# write.csv(test.set, "/Users/timmorales/Desktop/STAT 6341/Tommy John Project/Data/testset.csv") 
# 
# write.csv(train.set, "/Users/timmorales/Desktop/STAT 6341/Tommy John Project/Data/trainset.csv") 

#set y and remove from test set
y<-as.factor(test.set$TJ_Yes)
test.set<- test.set %>%
  select(-c("TJ_Yes"))

#make factor
train.set$TJ_Yes<-as.factor(train.set$TJ_Yes)


#check balance 
prop.table(table(train.set$TJ_Yes))
table(train.set$TJ_Yes)

#make factor before i SMOTE 
train.set[,c(91:336)]<- lapply(train.set[,c(91:336)], as.factor)
set.seed(23)
trainSplit <- SMOTE(TJ_Yes ~ ., train.set, perc.over = 1000, perc.under = 350)

#perc over /100 is how many times u randomly sample minority 
#perc under/ 100 is ratio of majority to minority you allow 

#i select this because it is just like oversampling the majority and leaving minority

table(trainSplit$TJ_Yes)

prop.table(table(trainSplit$TJ_Yes))

# write.csv(trainSplit,"/Users/timmorales/Desktop/STAT 6341/Tommy John Project/Data/SMOTETRAIN.csv")


```

I used Uniform Manifold Approximation Projection to visualize data and reduce dimensions to asses clustering effect. 
```{r}
library(umap)
as.numeric.factor <- function(x) {as.numeric(levels(x))[x]}
map.df <- trainSplit[,-332]
map.df[,91:335]<-lapply(trainSplit[,91:335], as.numeric.factor)

df.map<-umap(map.df)
umap.plot<-function(x, labels,
          main="A UMAP visualization of the Tommy John dataset",
          colors=c("grey", "red"),
          pad=0.1, alpha=.5, cex=0.65, pch=19, add=FALSE, legend.suffix="",
          cex.main=1, cex.legend=1) {
         layout = x   
         if (is(x, "umap")) {     layout = x$layout
         } 
   
       xylim = range(layout)
       xylim = xylim + ((xylim[2]-xylim[1])*pad)*c(-0.5, 0.5)
      if (!add) {
      par(mar=c(0.2,0.7,1.2,0.7), ps=10)
     plot(xylim, xylim, type="n", axes=F, frame=F)
     rect(xylim[1], xylim[1], xylim[2], xylim[2], border="#aaaaaa", lwd=0.25)  
   }
   points(layout[,1], layout[,2], col=colors[as.integer(labels)],
          cex=cex, pch=pch)
   mtext(side=3, main, cex=cex.main)

   labels.u = unique(labels)
   legend.pos = "topright"
  legend.text = as.character(labels.u)
   if (add) {
     legend.pos = "bottomright"
     legend.text = paste(as.character(labels.u), legend.suffix)
   }
   legend(legend.pos, legend=legend.text,
          col=colors[as.integer(labels.u)],
          bty="n", pch=pch, cex=cex.legend)
}

umap.plot(df.map,as.factor(trainSplit$TJ_Yes))

```

UMAP IF I DONT USE SMOTE

```{r}
train.map.df <- train.set[,-332]
train.map.df[,91:335]<-lapply(train.set[,91:335], as.numeric.factor)

df.map<-umap(train.map.df)

umap.plot(df.map,as.factor(train.set$TJ_Yes))



```

Test set UMAP
```{r}
test.map.df<-test.set
df.map<-umap(test.map.df)

umap.plot(df.map,as.factor(y))

```

The interesting take away from UMAP is that when we use SMOTE, we see to lose the overarching structure in the data. This can be seen in how the train and test splits both have similar structure but the UMAP is different for the SMOTE. 

This is raises concerns about the functionality of smote in this dataset. 





```{r}

levels(trainSplit$TJ_Yes) <- c("No", "Yes")
levels(y) <- c("No", "Yes")

```


# RF Optimization W SMOTE


```{r}
library(ranger)
##Define hypergrid
hyper_grid <- expand.grid(
  mtry = floor((335) * c(.05, .15, .25, .333, .4)),
  min.node.size = c(1, 3, 5, 10), 
  replace = c(TRUE, FALSE),                               
  sample.fraction = c(.5, .75, 1),
  cutoff = c(0.05, 0.07, 0.10,0.15, 0.5)
)
cuts = c(0.10, 0.15, 0.3, 0.4, 0.5)


end<-nrow(hyper_grid)


loop<-seq(1,end,5)

##Full cartesian grid search
for(i in loop) {
  ##ith hyperparameter combination
  fit <- ranger(
    formula         = TJ_Yes ~ ., 
    data            = trainSplit, 
    num.trees       = (3350),
    mtry            = hyper_grid$mtry[i],
    min.node.size   = hyper_grid$min.node.size[i],
    replace         = hyper_grid$replace[i],
    sample.fraction = hyper_grid$sample.fraction[i],
    verbose         = F,
    seed            = 23,
    respect.unordered.factors = FALSE,
    num.threads     = 3,
    splitrule = "gini",
    classification = T,
    probability = T
  )
    for (j in 1:length(cuts)){
    hyper_grid$cutoff[i+j-1]<- cuts[j]
    fit.pred<- as.factor(ifelse(fit$predictions[,2]>cuts[j],"Yes","No"))
    hyper_grid$sens[i+j-1] <- sensitivity(fit.pred, trainSplit$TJ_Yes, positive = "Yes")
    hyper_grid$NPV[i+j-1] <- negPredValue(fit.pred, trainSplit$TJ_Yes, positive = "Yes")
  #making my own F1 score type but giving more weight to recall
  # i want a balance between recall and postive pred value
  hyper_grid$Score[i+j-1] <-  1.5*(hyper_grid$sens[i+j-1]) + hyper_grid$NPV[i+j-1]
    }
}


NPV_grid <- hyper_grid %>%
  arrange(desc(NPV),desc(Score),desc(sens)) %>%
  head(10)


head(NPV_grid)
```

## Optimized on NPV 

```{r}
opt.fit <- ranger(
    formula         = TJ_Yes ~ ., 
    data            = trainSplit, 
    num.trees       = 4000,
    mtry            = NPV_grid$mtry[1],
    min.node.size   = NPV_grid$min.node.size[1],
    replace         = NPV_grid$replace[1],
    sample.fraction = NPV_grid$sample.fraction[1],
    verbose         = TRUE,
    seed            = 23,
    respect.unordered.factors = FALSE,
    num.threads     = 3,
    splitrule = "gini",
    classification = T,
    probability = T
  )

train.pred.rf<- as.factor(ifelse(opt.fit$predictions[,2]>NPV_grid$cutoff[1],"Yes","No"))
confusionMatrix(train.pred.rf, trainSplit$TJ_Yes)

```

### Test Set Results Optimized on NPV

```{r}
opt.rf.pred = predict(opt.fit, test.set)

classification.pred.rf<- as.factor(ifelse(opt.rf.pred$predictions[,2]>NPV_grid$cutoff[1],"Yes","No"))
confusionMatrix(classification.pred.rf, y)

```

## Optimized on Recall 

```{r}
recall_grid<-hyper_grid %>%
  arrange(desc(sens),desc(Score), desc(NPV)) %>%
  head(10)

head(recall_grid)
```

```{r}
recall.fit <- ranger(
    formula         = TJ_Yes ~ ., 
    data            = trainSplit, 
    num.trees       = 4000,
    mtry            = recall_grid$mtry[1],
    min.node.size   = recall_grid$min.node.size[1],
    replace         = recall_grid$replace[1],
    sample.fraction = recall_grid$sample.fraction[1],
    verbose         = TRUE,
    seed            = 23,
    respect.unordered.factors = FALSE,
    num.threads     = 3,
    splitrule = "gini",
    classification = T,
    probability = T
  )



recall.rf<- as.factor(ifelse(recall.fit$predictions[,2]>recall_grid$cutoff[2],"Yes","No"))
confusionMatrix(recall.rf, trainSplit$TJ_Yes)

```

### Test Set Results Optimized on Recall

```{r}
opt.recall = predict(recall.fit, test.set)

recall.pred.rf<- as.factor(ifelse(opt.recall$predictions[,2]>recall_grid$cutoff[1],"Yes","No"))
confusionMatrix(recall.pred.rf, y)

```


## Optimized on my custom score 

```{r}

score_grid<-hyper_grid %>%
  arrange(desc(Score), desc(sens), desc(NPV)) %>%
  head(10)

head(score_grid)
```

```{r}
score.fit <- ranger(
    formula         = TJ_Yes ~ ., 
    data            = trainSplit, 
    num.trees       = 4000,
    mtry            = score_grid$mtry[1],
    min.node.size   = score_grid$min.node.size[1],
    replace         = score_grid$replace[1],
    sample.fraction = score_grid$sample.fraction[1],
    verbose         = TRUE,
    seed            = 23,
    respect.unordered.factors =FALSE,
    num.threads     = 3,
    splitrule = "gini",
    classification = T,
    probability = T
  )



score.rf<- as.factor(ifelse(score.fit$predictions[,2]>score_grid$cutoff[1],"Yes","No"))
confusionMatrix(score.rf, trainSplit$TJ_Yes)

```

### Testset results with custom score 

```{r}
opt.score = predict(opt.fit, test.set)

score.pred.rf<- as.factor(ifelse(opt.score$predictions[,2]>score_grid$cutoff[1],"Yes","No"))
confusionMatrix(score.pred.rf, y)

```



# RF Optimizied Just Train Set

```{r}

levels(train.set$TJ_Yes) <- c("No", "Yes")
levels(y) <- c("No", "Yes")

```



```{r}
library(ranger)
##Define hypergrid
hyper_grid <- expand.grid(
  mtry = floor((337) * c(.05, .15, .25, .333, .4)),
  min.node.size = c(1, 3, 5, 10), 
  replace = c(TRUE, FALSE),                               
  sample.fraction = c(.5, .75, 1),
  cutoff = c(0.05, 0.07, 0.10,0.15, 0.5)
)
cuts = c(0.10, 0.15, 0.3, 0.4, 0.5)


end<-nrow(hyper_grid)

loop<-seq(1,end,5)

##Full cartesian grid search
for(i in loop) {
  ##ith hyperparameter combination
  fit <- ranger(
    formula         = TJ_Yes ~ ., 
    data            = train.set, 
    num.trees       = (3370),
    mtry            = hyper_grid$mtry[i],
    min.node.size   = hyper_grid$min.node.size[i],
    replace         = hyper_grid$replace[i],
    sample.fraction = hyper_grid$sample.fraction[i],
    verbose         = F,
    seed            = 23,
    respect.unordered.factors = 'order',
    num.threads     = 3,
    splitrule = "gini",
    classification = T,
    probability = T
  )
    for (j in 1:length(cuts)){
    hyper_grid$cutoff[i+j-1]<- cuts[j]
    fit.pred<- as.factor(ifelse(fit$predictions[,2]>cuts[j],"Yes","No"))
    hyper_grid$sens[i+j-1] <- sensitivity(fit.pred, train.set$TJ_Yes, positive = "Yes")
    hyper_grid$NPV[i+j-1] <- negPredValue(fit.pred, train.set$TJ_Yes, positive = "Yes")
  #making my own F1 score type but giving more weight to recall
  # i want a balance between recall and postive pred value
  hyper_grid$Score[i+j-1] <-  1.5*(hyper_grid$sens[i+j-1]) + hyper_grid$NPV[i+j-1]
    }
}



NPV_grid <- hyper_grid %>%
  arrange(desc(NPV),desc(Score),desc(sens)) %>%
  head(10)


head(NPV_grid)
```

## Optimized on NPV 

```{r}
opt.fit <- ranger(
    formula         = TJ_Yes ~ ., 
    data            = train.set, 
    num.trees       = 4000,
    mtry            = NPV_grid$mtry[1],
    min.node.size   = NPV_grid$min.node.size[1],
    replace         = NPV_grid$replace[1],
    sample.fraction = NPV_grid$sample.fraction[1],
    verbose         = TRUE,
    seed            = 23,
    respect.unordered.factors = F,
    num.threads     = 3,
    splitrule = "gini",
    classification = T,
    probability = T
  )



train.pred.rf<- as.factor(ifelse(opt.fit$predictions[,2] > NPV_grid$cutoff[1],"Yes","No"))
confusionMatrix(train.pred.rf, train.set$TJ_Yes)

```

### Test Set Results Optimized on NPV

```{r}
opt.rf.pred = predict(opt.fit, test.set)

classification.pred.rf<- as.factor(ifelse(opt.rf.pred$predictions[,2]>NPV_grid$cutoff[1],"Yes","No"))
confusionMatrix(classification.pred.rf, y)

```

## Optimized on Recall 

```{r}

recall_grid <- hyper_grid%>%
  arrange(desc(sens)) %>%
  head(10)

head(recall_grid)
```

```{r}
recall.fit <- ranger(
    formula         = TJ_Yes ~ ., 
    data            = train.set, 
    num.trees       = 4000,
    mtry            = recall_grid$mtry[1],
    min.node.size   = recall_grid$min.node.size[1],
    replace         = recall_grid$replace[1],
    sample.fraction = recall_grid$sample.fraction[1],
    verbose         = TRUE,
    seed            = 23,
    respect.unordered.factors = F,
    num.threads     = 3,
    splitrule = "gini",
    classification = T,
    probability = T
  )



recall.rf<- as.factor(ifelse(recall.fit$predictions[,2]>recall_grid$cutoff[1],"Yes","No"))
confusionMatrix(recall.rf, train.set$TJ_Yes)

```

### Test Set Results Optimized on Recall

```{r}
opt.recall = predict(recall.fit, test.set)

recall.pred.rf<- as.factor(ifelse(opt.recall$predictions[,2]>recall_grid$cutoff[1],"Yes","No"))
confusionMatrix(recall.pred.rf, y)

```


## Optimized on my custom score 

```{r}

score_grid<-hyper_grid %>%
  arrange(desc(Score)) %>%
  head(10)

head(score_grid)
```

```{r}
score.fit <- ranger(
    formula         = TJ_Yes ~ ., 
    data            = train.set, 
    num.trees       = 4000,
    mtry            = score_grid$mtry[1],
    min.node.size   = score_grid$min.node.size[1],
    replace         = score_grid$replace[1],
    sample.fraction = score_grid$sample.fraction[1],
    verbose         = TRUE,
    seed            = 23,
    respect.unordered.factors = F,
    num.threads     = 3,
    splitrule = "gini",
    classification = T,
    probability = T
  )


score.rf<- as.factor(ifelse(score.fit$predictions[,2]>score_grid$cutoff[1],"Yes","No"))
confusionMatrix(score.rf, train.set$TJ_Yes)

```

### Testset results with custom score 

```{r}
opt.score = predict(opt.fit, test.set)

score.pred.rf<- as.factor(ifelse(opt.score$predictions[,2]>score_grid$cutoff[1],"Yes","No"))
confusionMatrix(score.pred.rf, y)

```












XGB


logistic reg

```{r}
library(glmnet)
ctrl <- trainControl(method="cv", number=10, classProbs = T)
set.seed(23) 
#making logit df
logit.trainsplit <- trainSplit


logit.trainsplit[,c(87:332,334:337)] <- lapply(logit.trainsplit[,c(87:332,334:337)], as.numeric)

logit.test <- test.set
logit.test[,c(87:336)] <- lapply(logit.test[,c(87:336)], as.numeric.factor )



#running basic logit 
logitmodel <- train(TJ_Yes ~., data = logit.trainsplit, method = "glm", trControl = ctrl, family = "binomial")


logitmodel

confusionMatrix(logitmodel)

train.pred<-predict(logitmodel, logit.trainsplit)

confusionMatrix(train.pred, trainSplit$TJ_Yes, positive = "Yes")

preds<-predict(logitmodel, logit.test, prob = T)

preds<- ifelse(preds.$Yes > .99, "Yes", "No")

levels(y)
levels(y) <- c("No", "Yes")
confusionMatrix(as.factor(preds), y, positive = "Yes")


```

What If We Use Variables Relating to only the pitcher.

```{r}
index<-c(8:19,22,27,30:35,37:40,80)
batter_dependent <- colnames(trainSplit[,c(8:19,22,27,30:35,37:40,80)])
trainSplit.2 <- trainSplit[,-index]
levels(trainSplit.2$TJ_Yes) <- c("No", "Yes")
test.set.2 <- test.set[,-index]



```

```{r}
library(ranger)
##Define hypergrid
hyper_grid <- expand.grid(
  mtry = floor((337) * c(.05, .15, .25, .333, .4)),
  min.node.size = c(1, 3, 5, 10), 
  replace = c(TRUE, FALSE),                               
  sample.fraction = c(.5, .75, 1),
  cutoff = c(0.05, 0.07, 0.10,0.15, 0.5)
)
cuts = c(0.10, 0.15, 0.3, 0.4, 0.5)


end<-nrow(hyper_grid)


loop<-seq(1,end,5)

##Full cartesian grid search
for(i in loop) {
  ##ith hyperparameter combination
  fit <- ranger(
    formula         = TJ_Yes ~ ., 
    data            = trainSplit.2, 
    num.trees       = (3110),
    mtry            = hyper_grid$mtry[i],
    min.node.size   = hyper_grid$min.node.size[i],
    replace         = hyper_grid$replace[i],
    sample.fraction = hyper_grid$sample.fraction[i],
    verbose         = F,
    seed            = 23,
    respect.unordered.factors = FALSE,
    num.threads     = 3,
    splitrule = "gini",
    classification = T,
    probability = T
  )
    for (j in 1:length(cuts)){
    hyper_grid$cutoff[i+j-1]<- cuts[j]
    fit.pred<- as.factor(ifelse(fit$predictions[,2]>cuts[j],"Yes","No"))
    hyper_grid$sens[i+j-1] <- sensitivity(fit.pred, trainSplit.2$TJ_Yes, positive = "Yes")
    hyper_grid$NPV[i+j-1] <- negPredValue(fit.pred, trainSplit.2$TJ_Yes, positive = "Yes")
  #making my own F1 score type but giving more weight to recall
  # i want a balance between recall and postive pred value
  hyper_grid$Score[i+j-1] <-  1.5*(hyper_grid$sens[i+j-1]) + hyper_grid$NPV[i+j-1]
    }
}


NPV_grid <- hyper_grid %>%
  arrange(desc(NPV),desc(Score),desc(sens)) %>%
  head(10)


head(NPV_grid)
```

## Optimized on NPV 

```{r}
opt.fit <- ranger(
    formula         = TJ_Yes ~ ., 
    data            = trainSplit.2, 
    num.trees       = 4000,
    mtry            = NPV_grid$mtry[1],
    min.node.size   = NPV_grid$min.node.size[1],
    replace         = NPV_grid$replace[1],
    sample.fraction = NPV_grid$sample.fraction[1],
    verbose         = TRUE,
    seed            = 23,
    respect.unordered.factors = FALSE,
    num.threads     = 3,
    splitrule = "gini",
    classification = T,
    probability = T
  )

train.pred.rf<- as.factor(ifelse(opt.fit$predictions[,2]>NPV_grid$cutoff[1],"Yes","No"))
confusionMatrix(train.pred.rf, trainSplit.2$TJ_Yes)

```

### Test Set Results Optimized on NPV

```{r}
opt.rf.pred = predict(opt.fit, test.set)

classification.pred.rf<- as.factor(ifelse(opt.rf.pred$predictions[,2]>NPV_grid$cutoff[1],"Yes","No"))
confusionMatrix(classification.pred.rf, y)

```


## Optimized on Recall 

```{r}

recall_grid <- hyper_grid%>%
  arrange(desc(sens)) %>%
  head(10)

head(recall_grid)
```

```{r}
recall.fit <- ranger(
    formula         = TJ_Yes ~ ., 
    data            = trainSplit.2, 
    num.trees       = 4000,
    mtry            = recall_grid$mtry[1],
    min.node.size   = recall_grid$min.node.size[1],
    replace         = recall_grid$replace[1],
    sample.fraction = recall_grid$sample.fraction[1],
    verbose         = TRUE,
    seed            = 23,
    respect.unordered.factors = F,
    num.threads     = 3,
    splitrule = "gini",
    classification = T,
    probability = T,
    importance = "impurity"
  )



recall.rf<- as.factor(ifelse(recall.fit$predictions[,2]>recall_grid$cutoff[1],"Yes","No"))
confusionMatrix(recall.rf, trainSplit.2$TJ_Yes)

```

### Test Set Results Optimized on Recall

```{r}
opt.recall = predict(recall.fit, test.set)

recall.pred.rf<- as.factor(ifelse(opt.recall$predictions[,2]>recall_grid$cutoff[1],"Yes","No"))
confusionMatrix(recall.pred.rf, y)

```

```{r}
varimp.recall <- recall.fit$variable.importance
recall.imp.df<- data.frame(variable = names(varimp.recall), importance = varimp.recall)
recall.imp.df <- recall.imp.df %>% 
  arrange(desc(importance))

ggplot(recall.imp.df[1:20,], aes(x=reorder(variable,importance), y=importance,fill=importance))+ 
      geom_bar(stat="identity", position="dodge")+ coord_flip()+
      ylab("Variable Importance")+
      xlab("")+
      ggtitle("Information Value Summary")+
      guides(fill=F)+
      scale_fill_gradient(low="tan", high="red")

```
## Optimized on my custom score 

```{r}

score_grid<-hyper_grid %>%
  arrange(desc(Score)) %>%
  head(10)

head(score_grid)
```

```{r}
score.fit <- ranger(
    formula         = TJ_Yes ~ ., 
    data            = trainSplit.2, 
    num.trees       = 4000,
    mtry            = score_grid$mtry[1],
    min.node.size   = score_grid$min.node.size[1],
    replace         = score_grid$replace[1],
    sample.fraction = score_grid$sample.fraction[1],
    verbose         = TRUE,
    seed            = 23,
    respect.unordered.factors = F,
    num.threads     = 3,
    splitrule = "gini",
    classification = T,
    probability = T, 
    importance = "impurity"
  )


score.rf<- as.factor(ifelse(score.fit$predictions[,2]>score_grid$cutoff[1],"Yes","No"))
confusionMatrix(score.rf, trainSplit.2$TJ_Yes)

```

### Testset results with custom score 

```{r}
opt.score = predict(opt.fit, test.set)

score.pred.rf<- as.factor(ifelse(opt.score$predictions[,2]>score_grid$cutoff[1],"Yes","No"))
confusionMatrix(score.pred.rf, y)

```
```{r}
varimp.score <- score.fit$variable.importance
score.imp.df<- data.frame(variable = names(varimp.score), importance = varimp.score)
score.imp.df <- recall.imp.df %>% 
  arrange(desc(importance))

ggplot(score.imp.df[1:20,], aes(x=reorder(variable,importance), y=importance,fill=importance))+ 
      geom_bar(stat="identity", position="dodge")+ coord_flip()+
      ylab("Variable Importance")+
      xlab("")+
      ggtitle("Information Value Summary")+
      guides(fill=F)+
      scale_fill_gradient(low="tan", high="red")
```

