---
title: "TJ Modeling"
author: "Tim Morales"
date: "11/20/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
setwd("/Users/timmorales/Desktop/STAT 6341/Tommy John Project/Data/")
df<-read.csv("ModelReadyData.csv")
df<- df[,-1]
```

SMOTE for class imbalance and test train split

```{r}
library(DMwR)
library(caret)
library(umap)
library(dplyr)
library(recipes)
set.seed(23)
train.indx <- createDataPartition(df$TJ_Yes, p = .7, list = F)

drop.vars<-c("Years.Between", "Repeat_Injury","TJ_No" )

train.set<-data.frame(df[train.indx, -which(names(df) %in% drop.vars)])

test.set<-data.frame(df[-train.indx,-which(names(df) %in% drop.vars)])
```


```{r}

y<-as.factor(test.set$TJ_Yes)
test.set<- test.set %>%
  select(-c("TJ_Yes"))

#make factor
train.set$TJ_Yes<-as.factor(train.set$TJ_Yes)


#check balance 
prop.table(table(train.set$TJ_Yes))
table(train.set$TJ_Yes)

trainSplit <- SMOTE(TJ_Yes ~ ., train.set, perc.over = 1000, perc.under = 350)

#perc over /100 is how many times u randomly sample minority 
#perc under/ 100 is ratio of majority to minority you allow 

#i select this because it is just like oversampling the majority and leaving minority

table(trainSplit$TJ_Yes)

prop.table(table(trainSplit$TJ_Yes))
```

I used Uniform Manifold Approximation Projection to visualize data and reduce dimensions to asses clustering effect. 
```{r}
library(umap)


df.map<-umap(trainSplit[,-c(333)])
umap.plot<-function(x, labels,
          main="A UMAP visualization of the Tommy John dataset",
          colors=c("grey", "red"),
          pad=0.1, alpha=.5, cex=0.65, pch=19, add=FALSE, legend.suffix="",
          cex.main=1, cex.legend=1) {
         layout = x   
         if (is(x, "umap")) {     layout = x$layout
         } 
   
       xylim = range(layout)
       xylim = xylim + ((xylim[2]-xylim[1])*pad)*c(-0.5, 0.5)
      if (!add) {
      par(mar=c(0.2,0.7,1.2,0.7), ps=10)
     plot(xylim, xylim, type="n", axes=F, frame=F)
     rect(xylim[1], xylim[1], xylim[2], xylim[2], border="#aaaaaa", lwd=0.25)  
   }
   points(layout[,1], layout[,2], col=colors[as.integer(labels)],
          cex=cex, pch=pch)
   mtext(side=3, main, cex=cex.main)

   labels.u = unique(labels)
   legend.pos = "topright"
  legend.text = as.character(labels.u)
   if (add) {
     legend.pos = "bottomright"
     legend.text = paste(as.character(labels.u), legend.suffix)
   }
   legend(legend.pos, legend=legend.text,
          col=colors[as.integer(labels.u)],
          bty="n", pch=pch, cex=cex.legend)
}

umap.plot(df.map,as.factor(trainSplit$TJ_Yes))

```

UMAP IF I DONT USE SMOTE

```{r}
df.map<-umap(train.set[,-c(333)])

umap.plot(df.map,as.factor(train.set$TJ_Yes))



```

Test set UMAP
```{r}
df_test_map<-umap(test.set)

umap.plot(df.map,as.factor(y))


```

The interesting take away from UMAP is that when we use SMOTE, we see to lose the overarching structure in the data. This can be seen in how the train and test splits both have similar structure but the UMAP is different for the SMOTE. 

This is raises concerns about the functionality of smote in this dataset. 





```{r}

levels(trainSplit$TJ_Yes) <- c("No", "Yes")
levels(y) <- c("No", "Yes")

```


# RF Optimization W SMOTE


```{r}
library(ranger)
##Define hypergrid
hyper_grid <- expand.grid(
  mtry = floor((337) * c(.05, .15, .25, .333, .4)),
  min.node.size = c(1, 3, 5, 10), 
  replace = c(TRUE, FALSE),                               
  sample.fraction = c(.5, .75, 1),
  cutoff = c(0.05, 0.07, 0.10,0.15, 0.5)
)
cuts = c(0.10, 0.15, 0.3, 0.4, 0.5)


end<-nrow(hyper_grid)


loop<-seq(1,end,5)

##Full cartesian grid search
for(i in loop) {
  ##ith hyperparameter combination
  fit <- ranger(
    formula         = TJ_Yes ~ ., 
    data            = trainSplit, 
    num.trees       = (337),
    mtry            = hyper_grid$mtry[i],
    min.node.size   = hyper_grid$min.node.size[i],
    replace         = hyper_grid$replace[i],
    sample.fraction = hyper_grid$sample.fraction[i],
    verbose         = F,
    seed            = 23,
    respect.unordered.factors = FALSE,
    num.threads     = 3,
    splitrule = "gini",
    classification = T,
    probability = T
  )
    for (j in 1:length(cuts)){
    hyper_grid$cutoff[i+j-1]<- cuts[j]
    fit.pred<- as.factor(ifelse(fit$predictions[,2]>cuts[j],"Yes","No"))
    hyper_grid$sens[i+j-1] <- sensitivity(fit.pred, trainSplit$TJ_Yes, positive = "Yes")
    hyper_grid$NPV[i+j-1] <- negPredValue(fit.pred, trainSplit$TJ_Yes, positive = "Yes")
  #making my own F1 score type but giving more weight to recall
  # i want a balance between recall and postive pred value
  hyper_grid$Score[i+j-1] <-  1.5*(hyper_grid$sens[i]) + hyper_grid$NPV[i]
    }
}

end_time <- Sys.time()

##Time elapsed
end_time - start_time

NPV_grid <- hyper_grid %>%
  arrange(desc(NPV),desc(Score),desc(sens)) %>%
  head(10)


head(NPV_grid)
```

## Optimized on NPV 

```{r}
opt.fit <- ranger(
    formula         = TJ_Yes ~ ., 
    data            = trainSplit, 
    num.trees       = (337),
    mtry            = NPV_grid$mtry[1],
    min.node.size   = NPV_grid$min.node.size[1],
    replace         = NPV_grid$replace[1],
    sample.fraction = NPV_grid$sample.fraction[1],
    verbose         = TRUE,
    seed            = 23,
    respect.unordered.factors = FALSE,
    num.threads     = 3,
    splitrule = "gini",
    classification = T,
    probability = T
  )

train.pred.rf<- as.factor(ifelse(opt.fit$predictions[,2]>NPV_grid$cutoff[1],"Yes","No"))
confusionMatrix(train.pred.rf, trainSplit$TJ_Yes)

```

### Test Set Results Optimized on NPV

```{r}
opt.rf.pred = predict(opt.fit, test.set)

classification.pred.rf<- as.factor(ifelse(opt.rf.pred$predictions[,2]>NPV_grid$cutoff[1],"Yes","No"))
confusionMatrix(classification.pred.rf, y)

```

## Optimized on Recall 

```{r}
recall_grid<-hyper_grid %>%
  arrange(desc(sens),desc(Score), desc(NPV)) %>%
  head(10)

head(recall_grid)
```

```{r}
recall.fit <- ranger(
    formula         = TJ_Yes ~ ., 
    data            = trainSplit, 
    num.trees       = 4000,
    mtry            = recall_grid$mtry[1],
    min.node.size   = recall_grid$min.node.size[1],
    replace         = recall_grid$replace[1],
    sample.fraction = recall_grid$sample.fraction[1],
    verbose         = TRUE,
    seed            = 23,
    respect.unordered.factors = FALSE,
    num.threads     = 3,
    splitrule = "gini",
    classification = T,
    probability = T
  )



recall.rf<- as.factor(ifelse(recall.fit$predictions[,2]>recall_grid$cutoff[1],"Yes","No"))
confusionMatrix(recall.rf, trainSplit$TJ_Yes)

```

### Test Set Results Optimized on Recall

```{r}
opt.recall = predict(recall.fit, test.set)

recall.pred.rf<- as.factor(ifelse(opt.recall$predictions[,2]>recall_grid$cutoff[1],"Yes","No"))
confusionMatrix(recall.pred.rf, y)

```


## Optimized on my custom score 

```{r}

score_grid<-hyper_grid %>%
  arrange(desc(Score), desc(sens), desc(NPV)) %>%
  head(10)

head(score_grid)
```

```{r}
score.fit <- ranger(
    formula         = TJ_Yes ~ ., 
    data            = trainSplit, 
    num.trees       = 10*337,
    mtry            = score_grid$mtry[1],
    min.node.size   = score_grid$min.node.size[1],
    replace         = score_grid$replace[1],
    sample.fraction = score_grid$sample.fraction[1],
    verbose         = TRUE,
    seed            = 23,
    respect.unordered.factors =FALSE,
    num.threads     = 3,
    splitrule = "gini",
    classification = T,
    probability = T
  )



score.rf<- as.factor(ifelse(score.fit$predictions[,2]>score_grid$cutoff[1],"Yes","No"))
confusionMatrix(score.rf, trainSplit$TJ_Yes)

```

### Testset results with custom score 

```{r}
opt.score = predict(opt.fit, test.set)

score.pred.rf<- as.factor(ifelse(opt.score$predictions[,2]>score_grid$cutoff[1],"Yes","No"))
confusionMatrix(score.pred.rf, y)

```



# RF Optimizied Just Train Set

```{r eval=FALSE, include=TRUE}

levels(train.set$TJ_Yes) <- c("No", "Yes")
levels(y) <- c("No", "Yes")

```



```{r}
library(ranger)
##Define hypergrid
hyper_grid <- expand.grid(
  mtry = floor((337) * c(.05, .15, .25, .333, .4)),
  min.node.size = c(1, 3, 5, 10), 
  replace = c(TRUE, FALSE),                               
  sample.fraction = c(.5, .75, 1),
  cutoff = c(0.05, 0.07, 0.10,0.15, 0.5)
)
cuts = c(0.10, 0.15, 0.3, 0.4, 0.5)


end<-nrow(hyper_grid)

loop<-seq(1,end,5)

##Full cartesian grid search
for(i in loop) {
  ##ith hyperparameter combination
  fit <- ranger(
    formula         = TJ_Yes ~ ., 
    data            = train.set, 
    num.trees       = (337),
    mtry            = hyper_grid$mtry[i],
    min.node.size   = hyper_grid$min.node.size[i],
    replace         = hyper_grid$replace[i],
    sample.fraction = hyper_grid$sample.fraction[i],
    verbose         = F,
    seed            = 23,
    respect.unordered.factors = 'order',
    num.threads     = 3,
    splitrule = "gini",
    classification = T,
    probability = T
  )
    for (j in 1:length(cuts)){
    hyper_grid$cutoff[i+j-1]<- cuts[j]
    fit.pred<- as.factor(ifelse(fit$predictions[,2]>cuts[j],"Yes","No"))
    hyper_grid$sens[i+j-1] <- sensitivity(fit.pred, trainSplit$TJ_Yes, positive = "Yes")
    hyper_grid$NPV[i+j-1] <- negPredValue(fit.pred, trainSplit$TJ_Yes, positive = "Yes")
  #making my own F1 score type but giving more weight to recall
  # i want a balance between recall and postive pred value
  hyper_grid$Score[i+j-1] <-  1.5*(hyper_grid$sens[i]) + hyper_grid$NPV[i]
    }
}

end_time <- Sys.time()

##Time elapsed
end_time - start_time

NPV_grid <- hyper_grid %>%
  arrange(desc(NPV),desc(Score),desc(sens)) %>%
  head(10)


head(NPV_grid)
```

## Optimized on NPV 

```{r}
opt.fit <- ranger(
    formula         = TJ_Yes ~ ., 
    data            = train.set, 
    num.trees       = 10*length(337),
    mtry            = NPV_grid$mtry[1],
    min.node.size   = NPV_grid$min.node.size[1],
    replace         = NPV_grid$replace[1],
    sample.fraction = NPV_grid$sample.fraction[1],
    verbose         = TRUE,
    seed            = 23,
    respect.unordered.factors = 'order',
    num.threads     = 3,
    splitrule = "gini",
    classification = T,
    probability = T
  )



train.pred.rf<- as.factor(ifelse(opt.fit$predictions[,2] > NPV_grid$cutoff[1],"Yes","No"))
confusionMatrix(train.pred.rf, train.set$TJ_Yes)

```

### Test Set Results Optimized on NPV

```{r}
opt.rf.pred = predict(opt.fit, test.set)

classification.pred.rf<- as.factor(ifelse(opt.rf.pred$predictions[,2]>NPV_grid$cutoff[1],"Yes","No"))
confusionMatrix(classification.pred.rf, y)

```

## Optimized on Recall 

```{r}

recall_grid <- hyper_grid%>%
  arrange(desc(sens)) %>%
  head(10)

head(recall_grid)
```

```{r}
recall.fit <- ranger(
    formula         = TJ_Yes ~ ., 
    data            = train.set, 
    num.trees       = 10*length(337),
    mtry            = recall_grid$mtry[1],
    min.node.size   = recall_grid$min.node.size[1],
    replace         = recall_grid$replace[1],
    sample.fraction = recall_grid$sample.fraction[1],
    verbose         = TRUE,
    seed            = 23,
    respect.unordered.factors = 'order',
    num.threads     = 3,
    splitrule = "gini",
    classification = T,
    probability = T
  )



recall.rf<- as.factor(ifelse(recall.fit$predictions[,2]>recall_grid$cutoff[1],"Yes","No"))
confusionMatrix(recall.rf, train.set$TJ_Yes)

```

### Test Set Results Optimized on Recall

```{r}
opt.recall = predict(recall.fit, test.set)

recall.pred.rf<- as.factor(ifelse(opt.recall$predictions[,2]>recall_grid$cutoff[1],"Yes","No"))
confusionMatrix(recall.pred.rf, y)

```


## Optimized on my custom score 

```{r}

score_grid<-hyper_grid %>%
  arrange(desc(Score)) %>%
  head(10)

head(score_grid)
```

```{r}
score.fit <- ranger(
    formula         = TJ_Yes ~ ., 
    data            = train.set, 
    num.trees       = 10*length(337),
    mtry            = score_grid$mtry[1],
    min.node.size   = score_grid$min.node.size[1],
    replace         = score_grid$replace[1],
    sample.fraction = score_grid$sample.fraction[1],
    verbose         = TRUE,
    seed            = 23,
    respect.unordered.factors = 'order',
    num.threads     = 3,
    splitrule = "gini",
    classification = T,
    probability = T
  )


score.rf<- as.factor(ifelse(score.fit$predictions[,2]>score_grid$cutoff[1],"Yes","No"))
confusionMatrix(score.rf, train.set$TJ_Yes)

```

### Testset results with custom score 

```{r}
opt.score = predict(opt.fit, test.set)

score.pred.rf<- as.factor(ifelse(opt.score$predictions[,2]>score_grid$cutoff[1],"Yes","No"))
confusionMatrix(score.pred.rf, y)

```












XGB


logistic with lasso penalty 

```{r}
library(glmnet)

ctrl <- trainControl(method="repeatedcv", number=10, repeats=2, classProbs = T)
lambda <- seq(-3,3,25)
set.seed(123) 

y<- trainSplit$TJ_Yes

x = model.matrix(TJ_Yes~., trainSplit)[,-1]

cv.lasso <- cv.glmnet(x, trainSplit$TJ_Yes, alpha = 1, family = "binomial")

plot(cv.lasso)

```

```{r}
# Fit the final model on the training data
lasso.logit <- glmnet(x, y, alpha = 1, family = "binomial",
                lambda = cv.lasso$lambda.min)
lasso.logit$beta


x.test = model.matrix(TJ_Yes~., data.frame(df.juice[-train.indx,-which(names(df.juice) %in% drop.vars)]))[,-1]

lasso.logit.pred <- predict(lasso.logit, x.test)

predictions <- lasso.logit %>% predict(x.test) %>% as.vector()

pred <- as.factor(ifelse(predictions>0,1,0))

confusionMatrix(pred, y)

length(pred)
length(y)
```

```{r}


logitmodel <- train(TJ_Yes ~ ., data = trainSplit, method = "glm",trControl = ctrl, family = "binomial")


logitmodel

confusionMatrix(logitmodel)

train.pred<-predict(logitmodel, trainSplit)

confusionMatrix(train.pred, trainSplit$TJ_Yes, positive = "Yes")

preds.<-predict(logitmodel, test.set, type = "prob")

preds<- ifelse( preds.$Yes > .999, "Yes", "No")

levels(y)<- c("No", "Yes")

confusionMatrix(as.factor(preds), y, positive = "Yes")

levels(y)

levels(preds)
cutoffs <- seq(0.1,0.9,0.1)
accuracy <- NULL
for (i in seq(along = cutoffs)){
prediction <- ifelse(logitmodel$finalModel$fitted.values >= cutoffs[i], 1, 0) 
accuracy <- c(accuracy,length(which(y==prediction))/length(prediction)*100)
}

plot(cutoffs,accuracy)

pred<-predict(logitmodel, test.set,)

prop.table(table(pred))

confusionMatrix(pred, y, positive = "Yes")

```





